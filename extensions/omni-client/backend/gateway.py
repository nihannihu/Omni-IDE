# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ§  Omni-IDE â€” Hybrid Intelligence Gateway (v2.1)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
#  Two-tier routing â€” NO HuggingFace:
#    CLOUD  â†’  Gemini 1.5 Pro  (PRIMARY for all user tasks)
#    LOCAL  â†’  Auto-detected Ollama model (offline fallback only)
#
#  The gateway auto-detects which models Ollama has installed and
#  picks the best one. Small local models (3B) are too weak for
#  smolagents' structured tool-calling, so Cloud is always primary.
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import os
import re
import time
import logging
from enum import Enum
from dataclasses import dataclass, field
from typing import Optional

from dotenv import load_dotenv

logger = logging.getLogger(__name__)

# â”€â”€ Load environment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from config import ENV_PATH
    load_dotenv(ENV_PATH, override=True)
except ImportError:
    load_dotenv()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ModelTier(Enum):
    CLOUD = "cloud"
    LOCAL = "local"


@dataclass
class RoutingDecision:
    tier: ModelTier
    model_id: str
    reason: str
    trigger_keyword: Optional[str] = None
    context_size: int = 0
    is_fallback: bool = False
    latency_ms: float = 0.0


# Trigger words that indicate a complex, reasoning-heavy task.
COMPLEXITY_TRIGGERS = frozenset({
    "refactor", "architect", "design", "explain", "debug",
    "why", "analyze", "plan", "optimize", "review",
    "migrate", "security", "audit", "performance", "scale",
    "compare", "evaluate", "strategy", "tradeoff", "trade-off",
})

# Context size threshold (in estimated tokens).
CONTEXT_THRESHOLD = 4000

# Model identifiers (LiteLLM format)
GEMINI_PRO_MODEL = "gemini/gemini-2.5-flash"


# Preferred local models (in order of preference)
OLLAMA_PREFERRED_MODELS = [
    "qwen2.5-coder:7b",
    "qwen2.5-coder:3b",
    "qwen2.5-coder:1.5b",
    "qwen2.5:7b",
    "qwen2.5:3b",
    "llama3:latest",
    "codellama:latest",
]
OLLAMA_FALLBACK_MODEL = "ollama/qwen2.5-coder:3b"

# Environment keys
GEMINI_API_KEY_ENV = "GEMINI_API_KEY"
OLLAMA_BASE_URL_ENV = "OLLAMA_BASE_URL"
OLLAMA_DEFAULT_URL = "http://localhost:11434"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MODEL GATEWAY
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ModelGateway:
    """
    Intelligent model routing gateway for Omni-IDE.

    Cloud-Primary strategy â€” NO HuggingFace dependency:
      - ALL user tasks â†’ Gemini 1.5 Pro (Cloud) â€” reliable tool-calling
      - Gemini down?   â†’ Fallback to auto-detected Local Ollama model
      - Local 3B models are too small for smolagents protocol
    """

    def __init__(self):
        # â”€â”€ FORCE RELOAD .env (keys may have been saved after server start) â”€â”€
        try:
            from config import ENV_PATH
            load_dotenv(str(ENV_PATH), override=True)
        except ImportError:
            load_dotenv(override=True)

        raw_key = os.getenv(GEMINI_API_KEY_ENV, "")

        # â”€â”€ FALLBACK: Read .env directly if os.getenv missed it â”€â”€
        if not raw_key:
            try:
                from pathlib import Path
                from config import ENV_PATH as _env_path
                env_file = Path(_env_path)
                if env_file.exists():
                    for line in env_file.read_text(encoding="utf-8").splitlines():
                        line = line.strip()
                        if line.startswith("GEMINI_API_KEY") and "=" in line:
                            raw_key = line.split("=", 1)[1].strip().strip("'").strip('"')
                            if raw_key:
                                os.environ[GEMINI_API_KEY_ENV] = raw_key
                                logger.info("ğŸ”‘ GATEWAY: Loaded GEMINI_API_KEY directly from .env file")
                            break
            except Exception as e:
                logger.warning(f"âš ï¸ GATEWAY: Direct .env read failed: {e}")

        # Strip surrounding quotes that dotenv.set_key() may have added
        self.gemini_key = raw_key.strip("'").strip('"').strip() or None
        self.local_url = os.getenv(OLLAMA_BASE_URL_ENV, OLLAMA_DEFAULT_URL)
        self._routing_history: list[RoutingDecision] = []
        self._local_available: Optional[bool] = None
        self._last_health_check: float = 0.0
        self._health_check_ttl: float = 30.0

        # â”€â”€ AUTO-DETECT LOCAL MODEL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.local_model_id = self._detect_local_model()

        # â”€â”€ Pre-build cloud model for runtime fallback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self._cloud_model = None

        # Boot-time diagnostics
        print(f"ğŸ”‘ GATEWAY DEBUG: Gemini Key Present? {bool(self.gemini_key)}")
        logger.info("ğŸ§  GATEWAY: Hybrid Intelligence Engine v2.1 initialized.")
        logger.info(f"   â”œâ”€ Gemini API Key: {'âœ… Loaded' if self.gemini_key else 'âŒ Missing (Local-only mode)'}")
        logger.info(f"   â”œâ”€ Ollama URL: {self.local_url}")
        logger.info(f"   â”œâ”€ Local Model: {self.local_model_id or 'âŒ None detected'}")
        logger.info(f"   â””â”€ Context Threshold: {CONTEXT_THRESHOLD} tokens")

    def _detect_local_model(self) -> Optional[str]:
        """
        Query Ollama's /api/tags to find which models are actually installed.
        Returns the best available model in LiteLLM format.
        """
        try:
            import urllib.request
            import json
            req = urllib.request.Request(f"{self.local_url}/api/tags", method="GET")
            with urllib.request.urlopen(req, timeout=3) as resp:
                data = json.loads(resp.read())
                installed = [m["name"] for m in data.get("models", [])]
                logger.info(f"   â”œâ”€ Ollama installed models: {installed}")

                for preferred in OLLAMA_PREFERRED_MODELS:
                    if preferred in installed:
                        model_id = f"ollama/{preferred}"
                        logger.info(f"   â”œâ”€ âœ… Auto-detected: {model_id}")
                        return model_id

                if installed:
                    local_models = [m for m in installed if 'cloud' not in m]
                    if local_models:
                        model_id = f"ollama/{local_models[0]}"
                        logger.warning(f"   â”œâ”€ âš ï¸ No preferred model. Using: {model_id}")
                        return model_id

                logger.warning("   â”œâ”€ âŒ Ollama has no local models installed.")
                return None

        except Exception as e:
            logger.warning(f"   â”œâ”€ âŒ Ollama not reachable ({e}). Local unavailable.")
            return None

    def get_cloud_model(self):
        """Get or create a Gemini cloud model (for runtime fallback)."""
        if self._cloud_model is None and self.gemini_key:
            try:
                from smolagents import LiteLLMModel
                self._cloud_model = LiteLLMModel(
                    model_id=GEMINI_PRO_MODEL,
                    api_key=self.gemini_key,
                )
                logger.info(f"â˜ï¸  GATEWAY: Cloud fallback ready â†’ {GEMINI_PRO_MODEL}")
            except Exception as e:
                logger.error(f"âŒ GATEWAY: Cloud fallback init failed: {e}")
        return self._cloud_model

    # ----------------------------------------------------------
    # CORE ROUTING: get_brain()
    # ----------------------------------------------------------

    def get_brain(self, task_complexity: str = "AUTO", user_query: str = "", context_size: int = 0):
        """
        The Smart Router. Returns a ready-to-use LiteLLMModel.

        Args:
            task_complexity: "HIGH", "LOW", or "AUTO" (auto-detect from query).
            user_query: The user's request (used for auto-detection).
            context_size: Estimated token count of the current context.

        Returns:
            A smolagents.LiteLLMModel instance.
        """
        start = time.perf_counter()

        # Auto-detect complexity from query if set to AUTO
        if task_complexity == "AUTO" and user_query:
            task_complexity = self._classify_complexity(user_query, context_size)

        if task_complexity == "HIGH":
            decision = self._try_cloud(user_query, context_size)
        else:
            local_model = self.local_model_id or OLLAMA_FALLBACK_MODEL
            decision = RoutingDecision(
                tier=ModelTier.LOCAL,
                model_id=local_model,
                reason="Simple task â€” local model is optimal",
                context_size=context_size,
            )

        decision.latency_ms = (time.perf_counter() - start) * 1000
        self._routing_history.append(decision)
        self._log_decision(decision)

        return self._build_model(decision)

    def get_model_for_chat(self, user_query: str, file_content: str = ""):
        """
        Convenience method for the chat pipeline.
        Estimates context size from file content and routes accordingly.
        """
        context_tokens = len(file_content) // 4 if file_content else 0
        return self.get_brain(
            task_complexity="AUTO",
            user_query=user_query,
            context_size=context_tokens,
        )

    # ----------------------------------------------------------
    # COMPLEXITY CLASSIFIER
    # ----------------------------------------------------------

    def _classify_complexity(self, query: str, context_size: int) -> str:
        """Determine if a task is HIGH or LOW complexity."""
        query_lower = query.lower()

        # Condition A: Complexity trigger words
        for keyword in COMPLEXITY_TRIGGERS:
            pattern = rf'\b{re.escape(keyword)}\b'
            if re.search(pattern, query_lower):
                logger.info(f"ğŸ”€ ROUTER: Complex task detected ('{keyword}'). â†’ GEMINI PRO")
                return "HIGH"

        # Condition B: Large context window
        if context_size > CONTEXT_THRESHOLD:
            logger.info(f"ğŸ”€ ROUTER: Large context ({context_size} > {CONTEXT_THRESHOLD}). â†’ GEMINI PRO")
            return "HIGH"

        # Condition C: Default â€” simple
        logger.info("ğŸŸ¢ ROUTER: Simple task. â†’ LOCAL QWEN")
        return "LOW"

    # ----------------------------------------------------------
    # CLOUD ROUTER
    # ----------------------------------------------------------

    def _try_cloud(self, query: str, context_size: int) -> RoutingDecision:
        """Attempt to route to Gemini Pro. Falls back to local if unavailable."""
        if self.gemini_key:
            trigger = None
            query_lower = query.lower()
            for keyword in COMPLEXITY_TRIGGERS:
                if re.search(rf'\b{re.escape(keyword)}\b', query_lower):
                    trigger = keyword
                    break

            return RoutingDecision(
                tier=ModelTier.CLOUD,
                model_id=GEMINI_PRO_MODEL,
                reason=f"Complex task: '{trigger}'" if trigger else f"Large context ({context_size} tokens)",
                trigger_keyword=trigger,
                context_size=context_size,
            )
        else:
            local_model = self.local_model_id or OLLAMA_FALLBACK_MODEL
            logger.warning("âš ï¸ GATEWAY: No GEMINI_API_KEY. Falling back to Local.")
            return RoutingDecision(
                tier=ModelTier.LOCAL,
                model_id=local_model,
                reason="Gemini key missing â€” fallback to local",
                context_size=context_size,
                is_fallback=True,
            )

    # ----------------------------------------------------------
    # MODEL FACTORY
    # ----------------------------------------------------------

    def _build_model(self, decision: RoutingDecision):
        """Build the actual LiteLLMModel instance with automatic fallback."""
        from smolagents import LiteLLMModel

        # â”€â”€ Cloud (Gemini) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if decision.tier == ModelTier.CLOUD:
            try:
                print("ğŸ”€ ROUTING: Attempting Gemini Pro (Cloud)...")
                model = LiteLLMModel(
                    model_id=GEMINI_PRO_MODEL,
                    api_key=self.gemini_key,
                )
                logger.info(f"â˜ï¸  GATEWAY: Gemini Pro ready â†’ {GEMINI_PRO_MODEL}")
                return model
            except Exception as e:
                print(f"âš ï¸ CLOUD FAIL: {e}. Switching to Local...")
                logger.error(f"âŒ Gemini init failed: {e}. Falling back to Local.")
                # Fall through to local

        # â”€â”€ Local (auto-detected model via Ollama) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        local_model = self.local_model_id
        if not local_model:
            if self.gemini_key and decision.tier != ModelTier.CLOUD:
                print("âš ï¸ No local model. Trying Gemini Cloud...")
                try:
                    model = LiteLLMModel(
                        model_id=GEMINI_PRO_MODEL,
                        api_key=self.gemini_key,
                    )
                    logger.info(f"â˜ï¸  GATEWAY: No local model â†’ using Gemini Pro")
                    return model
                except Exception as e:
                    logger.error(f"âŒ Cloud fallback also failed: {e}")

            raise RuntimeError(
                "ğŸš¨ GATEWAY CRITICAL: No models available.\n"
                "Ollama has no compatible models and Gemini is unavailable.\n"
                "Fix: Run 'ollama pull qwen2.5-coder:3b' or add GEMINI_API_KEY to .env."
            )

        print(f"ğŸ”€ ROUTING: Using Local {local_model}...")
        try:
            model = LiteLLMModel(
                model_id=local_model,
                api_base=self.local_url,
            )
            logger.info(f"âš¡ GATEWAY: Local model ready â†’ {local_model}")
            return model
        except Exception as e:
            logger.error(f"âŒ Local init failed: {e}")
            if self.gemini_key:
                print("âš ï¸ Local failed. Falling back to Gemini Cloud...")
                try:
                    model = LiteLLMModel(
                        model_id=GEMINI_PRO_MODEL,
                        api_key=self.gemini_key,
                    )
                    logger.info(f"â˜ï¸  GATEWAY: Local failed â†’ using Gemini Pro")
                    return model
                except Exception as cloud_err:
                    logger.error(f"âŒ Cloud fallback also failed: {cloud_err}")

            raise RuntimeError(
                "ğŸš¨ GATEWAY CRITICAL: Both Local and Cloud models failed.\n"
                "Please ensure Ollama is running or add a valid GEMINI_API_KEY to .env."
            ) from e

    # ----------------------------------------------------------
    # HEALTH CHECK
    # ----------------------------------------------------------

    def _is_ollama_available(self) -> bool:
        """Check if Ollama is reachable (cached for 30s)."""
        now = time.time()
        if (now - self._last_health_check) < self._health_check_ttl and self._local_available is not None:
            return self._local_available

        try:
            import urllib.request
            req = urllib.request.Request(f"{self.local_url}/api/tags", method="GET")
            urllib.request.urlopen(req, timeout=2)
            self._local_available = True
        except Exception:
            self._local_available = False

        self._last_health_check = now
        return self._local_available

    # ----------------------------------------------------------
    # OBSERVABILITY
    # ----------------------------------------------------------

    def _log_decision(self, d: RoutingDecision):
        """Log a routing decision for observability."""
        icon = "â˜ï¸ " if d.tier == ModelTier.CLOUD else "âš¡"
        print(
            f"{icon} GATEWAY: {d.tier.value.upper()} â†’ {d.model_id} | "
            f"{d.reason} | ctx={d.context_size} | {d.latency_ms:.1f}ms"
        )

    def get_routing_stats(self) -> dict:
        """Return routing statistics for the /health endpoint."""
        total = len(self._routing_history)
        cloud = sum(1 for d in self._routing_history if d.tier == ModelTier.CLOUD)
        local = total - cloud
        avg_latency = (
            sum(d.latency_ms for d in self._routing_history) / total
            if total else 0
        )
        return {
            "total_routes": total,
            "cloud_routes": cloud,
            "local_routes": local,
            "avg_latency_ms": round(avg_latency, 2),
            "gemini_key_present": bool(self.gemini_key),
            "local_model": self.local_model_id,
            "ollama_available": self._local_available,
        }

    def select_model(self, *args, **kwargs):
        """Legacy compatibility shim."""
        return self.get_brain(*args, **kwargs)


# â”€â”€ Singleton â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_gateway() -> ModelGateway:
    global _gateway_instance
    if _gateway_instance is None:
        _gateway_instance = ModelGateway()
    return _gateway_instance

def reinitialize_gateway() -> ModelGateway:
    """Force-recreate the gateway (picks up new .env keys)."""
    global _gateway_instance
    _gateway_instance = None
    return get_gateway()

# â”€â”€ Dynamic Singleton Proxy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class GatewayProxy:
    """Ensures 'from gateway import model_gateway' always hits the latest instance."""
    def __getattr__(self, name):
        return getattr(get_gateway(), name)

model_gateway = GatewayProxy()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SELF-TEST
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
if __name__ == "__main__":
    gw = ModelGateway()

    test_cases = [
        ("refactor the auth module", "HIGH"),
        ("fix a typo", "LOW"),
        ("debug websocket", "HIGH"),
        ("add a comment", "LOW"),
        ("explain architecture", "HIGH"),
        ("hello", "LOW"),
        ("design database schema", "HIGH"),
        ("rename variable", "LOW"),
        ("optimize query performance", "HIGH"),
        ("update readme", "LOW"),
    ]

    print("\nğŸ§ª Gateway Self-Test:")
    correct = 0
    for query, expected in test_cases:
        result = gw._classify_complexity(query, 100)
        ok = result == expected
        correct += ok
        print(f"  {'âœ…' if ok else 'âŒ'} '{query}' â†’ {result} (expected {expected})")

    print(f"\n  Score: {correct}/{len(test_cases)}")
    print(f"  Stats: {gw.get_routing_stats()}")
