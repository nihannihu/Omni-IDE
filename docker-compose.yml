version: '3.8'

services:
  backend:
    build: ./backend
    container_name: omni-backend
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app # Live reload during development (optional)
      - huggingface_cache:/root/.cache/huggingface # Persist downloaded models
      - ./backend/.env:/app/.env # Mount .env file for secrets
    env_file:
      - ./backend/.env
    environment:
      - PYTHONUNBUFFERED=1
    # GPU support is optional for Cloud Brain, but useful for local Whisper/Transcriber
    # Uncomment below to enable GPU if available
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - capabilities: [gpu]
    restart: unless-stopped

  frontend:
    build:
      context: ./frontend
      args:
        # Default to localhost for local development
        # Change to your server IP/Domain for production
        NEXT_PUBLIC_BACKEND_URL: "ws://localhost:8000/ws/omni"
    container_name: omni-frontend
    ports:
      - "3000:3000"
    depends_on:
      - backend
    restart: unless-stopped

volumes:
  huggingface_cache:
