import os
import json
import uuid
import time
import hashlib
import difflib
import logging
import tempfile
from typing import Dict, Any, Optional
from pathlib import Path

logger = logging.getLogger(__name__)

class DiffStagingLayer:
    """
    Safely intercepts file writes generated by AI agents.
    Provides a staging mechanism to generate unified diffs
    and requires explicit approval before applying atomic patches.
    """
    def __init__(self, workspace_dir: str):
        self.workspace_dir = Path(workspace_dir).resolve() if workspace_dir else None
        # Sessions map: { session_id (str): SessionData (dict) }
        self.sessions: Dict[str, Dict[str, Any]] = {}
        
        # Optional persistence layer for staging data
        self.persist_file = self.workspace_dir / ".omni_staging.json" if self.workspace_dir else None
        self._load_sessions()

    def _load_sessions(self):
        """Loads persistent session data if it exists."""
        if self.persist_file and self.persist_file.exists():
            try:
                with open(self.persist_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    if isinstance(data, dict):
                        self.sessions = data
            except Exception as e:
                logger.warning(f"Failed to load staging persistence: {e}")

    def _save_sessions(self):
        """Saves session data to disk."""
        if self.persist_file:
            try:
                with open(self.persist_file, "w", encoding="utf-8") as f:
                    json.dump(self.sessions, f, indent=2)
            except Exception as e:
                logger.warning(f"Failed to save staging persistence: {e}")

    def _is_safe_path(self, filepath: Path) -> bool:
        if not self.workspace_dir:
            return False
        return str(filepath.resolve()).startswith(str(self.workspace_dir))

    def _compute_hash(self, content: str) -> str:
        return hashlib.sha256(content.encode('utf-8')).hexdigest()

    def create_patch(self, file_path: str, new_content: str) -> Dict[str, Any]:
        """
        Intercepts a write request, generates a unified diff, and creates a staging session.
        Returns the diff payload and a secure session_id.
        """
        if not self.workspace_dir:
            return {"error": "Workspace not loaded. Cannot propose patch."}

        # Normalize the path
        path_obj = Path(file_path)
        if not path_obj.is_absolute():
            path_obj = self.workspace_dir / path_obj
            
        path_obj = path_obj.resolve()
        
        if not self._is_safe_path(path_obj):
            logger.error(f"[SECURITY] Denied out-of-bounds patch proposal to {path_obj}")
            return {"error": "Security Block: Cannot propose patches outside workspace directory."}

        abs_path_str = str(path_obj)
        
        # Read original content if file exists
        original_content = ""
        original_hash = ""
        original_mtime = 0.0
        file_size = 0
        
        if path_obj.exists() and path_obj.is_file():
            try:
                # 1MB limit for diffing
                file_size = path_obj.stat().st_size
                if file_size > 1024 * 1024:
                    return {"error": "FULL_REPLACE_REQUIRED: File exceeds 1MB diffing limit."}
                    
                original_content = path_obj.read_text(encoding='utf-8')
                original_hash = self._compute_hash(original_content)
                original_mtime = path_obj.stat().st_mtime
            except UnicodeDecodeError:
                return {"error": "Cannot propose patches for binary files."}

        # Generate Unified Diff
        diff_lines = list(difflib.unified_diff(
            original_content.splitlines(keepends=True),
            new_content.splitlines(keepends=True),
            fromfile=f"a/{path_obj.name}",
            tofile=f"b/{path_obj.name}",
            n=3
        ))
        unified_diff = "".join(diff_lines)

        if not unified_diff.strip():
            return {"status": "unchanged", "message": "The proposed content exactly matches the existing file."}

        # Generate secure session ID
        session_id = str(uuid.uuid4())

        # Stage it into memory
        session_data = {
            "session_id": session_id,
            "file_path": abs_path_str,
            "original_hash": original_hash,
            "original_mtime": original_mtime,
            "proposed_content": new_content,
            "diff": unified_diff,
            "status": "PENDING",
            "created_at": time.time()
        }
        
        self.sessions[session_id] = session_data
        self._save_sessions()
        
        logger.info(f"[STAGING] Generated patch for {path_obj.name} ({len(unified_diff)} bytes of diff). Session: {session_id}")

        return {
            "status": "staged",
            "session_id": session_id,
            "file": abs_path_str,
            "filename": path_obj.name,
            "diff": unified_diff,
            "action": "modify" if path_obj.exists() else "create"
        }

    def get_active_sessions(self) -> list:
        """Returns a list of all PENDING sessions with core metadata."""
        active = []
        for sid, session in self.sessions.items():
            if session.get("status") == "PENDING":
                active.append({
                    "session_id": sid,
                    "file_path": session.get("file_path"),
                    "status": session.get("status"),
                    "created_at": session.get("created_at", 0)
                })
        # Sort by creation time descending (newest first)
        active.sort(key=lambda x: x["created_at"], reverse=True)
        return active

    def get_patch(self, session_id: str) -> Dict[str, Any]:
        """Returns session metadata and diff payload."""
        return self.sessions.get(session_id, {"error": "Session not found."})

    def apply_patch(self, session_id: str) -> Dict[str, Any]:
        """
        Atomically applies a staged patch to the filesystem.
        Validates hash and mtime to detect collision race conditions.
        """
        session = self.sessions.get(session_id)
        if not session:
            return {"error": f"Session {session_id} not found."}
            
        if session["status"] != "PENDING":
             return {"error": f"Session status is {session['status']}, must be PENDING."}

        absolute_path = session["file_path"]
        path_obj = Path(absolute_path)
        new_content = session["proposed_content"]
        
        # Collision Detection
        if path_obj.exists():
            current_mtime = path_obj.stat().st_mtime
            if current_mtime != session["original_mtime"]:
                 # File changed since patch proposal
                 current_content = path_obj.read_text(encoding='utf-8')
                 if self._compute_hash(current_content) != session["original_hash"]:
                     logger.warning(f"[CONFLICT] Rejecting patch {session_id}: File modified externally.")
                     return {"error": "Conflict Detected: The underlying file was modified after this patch was proposed."}
        
        try:
            # Atomic Write Strategy
            path_obj.parent.mkdir(parents=True, exist_ok=True)
            
            # Write to a secure temporary file first
            fd, temp_path = tempfile.mkstemp(dir=str(path_obj.parent), prefix=".diff_tmp_")
            with os.fdopen(fd, 'w', encoding='utf-8') as f:
                f.write(new_content)
                f.flush()
                os.fsync(f.fileno()) # Force write to disk

            # Atomically replace the target file
            os.replace(temp_path, absolute_path)
            
            # Mark Session as Applied
            session["status"] = "APPLIED"
            self._save_sessions()
            
            logger.info(f"[APPLIED] Patch applied atomically to {path_obj.name}")
            return {"status": "success", "file": absolute_path}
            
        except Exception as e:
            # [RECOVERY] Clean up temp file if something blew up
            if 'temp_path' in locals() and os.path.exists(temp_path):
                try:
                    os.remove(temp_path)
                except Exception:
                    pass
            logger.error(f"[ROLLBACK] Failed to apply patch atomically: {e}. File pristine.")
            return {"error": f"Atomic write failed: {str(e)}"}

    def discard_patch(self, session_id: str) -> Dict[str, str]:
        """Discards a staged patch from memory and marks it rejected."""
        session = self.sessions.get(session_id)
        if not session:
            return {"error": f"Session {session_id} not found."}
            
        if session["status"] != "PENDING":
            return {"error": f"Cannot discard session. Current status is {session['status']}."}
            
        session["status"] = "DISCARDED"
        # Release heavy memory buffers
        session["proposed_content"] = ""
        session["diff"] = "Discarded."
        
        self._save_sessions()
        logger.info(f"[STAGING] Session {session_id} discarded.")
        return {"status": "discarded", "message": "Patch rejected successfully."}

    def cleanup_expired(self, ttl_seconds: int = 3600):
        """Removes pending sessions older than TTL, and flushes applied/discarded states."""
        current_time = time.time()
        to_delete = []
        
        for sid, session in self.sessions.items():
            age = current_time - session.get("created_at", 0)
            
            # Delete if stale pending map, or if already resolved
            if (session["status"] == "PENDING" and age > ttl_seconds) or session["status"] in ["APPLIED", "DISCARDED"]:
                to_delete.append(sid)
                
        for sid in to_delete:
            del self.sessions[sid]
            
        if to_delete:
            self._save_sessions()
            logger.info(f"[CLEANUP] Deleted {len(to_delete)} expired/resolved staging sessions.")
